{"posts":[{"title":"国内开源镜像站点分享","content":"企业站点 搜狐：http://mirrors.sohu.com/ 腾讯：https://mirrors.cloud.tencent.com// 网易：http://mirrors.163.com/ 阿里：http://mirrors.aliyun.com/ 华为：https://mirrors.huaweicloud.com/ 教育站点 上海交通大学：http://ftp.sjtu.edu.cn/html/resources.xml（部分移动，无法访问） 华中科技大学：http://mirror.hust.edu.cn/（当前已用容量估计：4.83T） 清华大学：http://mirrors.tuna.tsinghua.edu.cn/（当前已用容量估计：9.8T） 北京理工大学：http://mirror.bit.edu.cn/web/ 兰州大学：http://mirror.lzu.edu.cn/ 中国科技大学：http://mirrors.ustc.edu.cn/（当前已用容量估计：21.32T） 大连东软信息学院：http://mirrors.neusoft.edu.cn/（当前已用容量估计：2.5T） 东北大学：http://mirror.neu.edu.cn/ 大连理工大学：http://mirror.dlut.edu.cn/ 哈尔滨工业大学：http://run.hit.edu.cn/html/（部分联通运营商出口状况不佳，无法访问） 北京交通大学：http://mirror.bjtu.edu.cn/cn/ 中国地质大学：http://mirrors.cug.edu.cn/（当前已用容量估计：2.3T） 浙江大学：http://mirrors.zju.edu.cn/ 厦门大学：http://mirrors.xmu.edu.cn/ 中山大学：http://mirror.sysu.edu.cn/ 重庆大学：http://mirrors.cqu.edu.cn/（当前已用容量估计：3.93T） 北京化工大学：http://ubuntu.buct.edu.cn/（AndroidSDK镜像仅供校内使用，当前已用容量估计：1.72T） 南阳理工学院：http://mirror.nyist.edu.cn/ 中国科学院：http://www.opencas.org/mirrors/ 电子科技大学星辰工作室：http://mirrors.stuhome.net/（当前已用容量估计：1.08T） 西北农林科技大学：http://mirrors.nwsuaf.edu.cn/（只做CentOS镜像，当前已用容量估计：140GB） 其他站 首都在线科技股份有限公司（英文名CapitalOnlineDataService）：http://mirrors.yun-idc.com/ 常州贝特康姆软件技术有限公司：http://centos.bitcomm.cn/（只做CentOS镜像，当前已用容量估计：140GB） 公云PubYun（母公司为贝特康姆）：http://mirrors.pubyun.com/ 软件版 操作系统类 Ubuntu 阿里云：http://mirrors.aliyun.com/ubuntu-releases/ 网易：http://mirrors.163.com/ubuntu-releases/ 搜狐：http://mirrors.sohu.com/ubuntu-releases/（搜狐在12年之后似乎不同步了） 首都在线科技股份有限公司：http://mirrors.yun-idc.com/ubuntu-releases/ CentOS 网易：http://mirrors.163.com/centos/ 搜狐：http://mirrors.sohu.com/centos/ 阿里云：http://mirrors.aliyun.com/centos/ 操作系统类 Tomcat、Apache 中国互联网络信息中心：http://mirrors.cnnic.cn/apache/ 华中科技大学：http://mirrors.hust.edu.cn/apache/ MySQL 搜狐：http://mirrors.sohu.com/mysql 中国科技技术大学：http://mirrors.ustc.edu.cn/mysql-ftp/Downloads PostgreSQL 浙江大学：http://mirrors.zju.edu.cn/postgresql/ MariaDB 中国科技技术大学：https://mirrors.ustc.edu.cn/mariadb/ VideoLAN 大连东软信息学院：http://mirrors.neusoft.edu.cn/videolan/ 中国科技大学：http://mirrors.ustc.edu.cn/videolan-ftp/ 开发工具类 Eclipse 中国科技大学：http://mirrors.ustc.edu.cn/eclipse/ 东北大学B：http://mirror.neu.edu.cn/eclipse/ 官方镜像列表状态地址 CentOS：http://mirror-status.centos.org/#cn Archlinux：https://www.archlinux.org/mirrors/status/ Ubuntu：https://launchpad.net/ubuntu/+cdmirrors Debian：http://mirror.debian.org/status.html FedoraLinux/FedoraEPEL：https://admin.fedoraproject.org/mirrormanager/mirrors Apache：http://www.apache.org/mirrors/#cn Cygwin：https://www.cygwin.com/mirrors.html 最后修改：2022 年 03 月 11 日 © 允许规范转载 打赏 赞赏作者 支付宝 微信 如果觉得我的文章对你有用，请随意赞赏 本文转自 https://gsbok.cn/59.html，如有侵权，请联系删除。 ","link":"https://zhiyong89.github.io/post/mirrors-china/"},{"title":"centos系列一键更换yum源脚本","content":"前言 脚本由本人所写,实际就是把命令整合了而已！ 仅适用于centos7.x系统 现已有1.1版本支持centos5-8 由于本人是是新手，脚本不太完善有能力的自行修改！ 注意事项 此脚本请使用root用户执行 脚本执行后会在etc下面创建一个备份原数据的文件夹为避免原数据丢失请不要删除此文件夹 有部分源是替换原文件内容实现更换yum源的,如果备份前自己就更换过源就会造成更换失败,所以不得已对此类源选择了从网上下载已经修改过的配置文件，这个好处就是避免了以上问题发生，坏处就是网络文件如果失效此类源去下载配置文件的时候会直接下载失败 如遇到下载失败报错的可以再次执行此脚本即可，注意首次执行此脚本会进行备份，第二次执行此脚本会报错备份失败，直接忽略即可! 演示 点我预览1.0版本 点我预览1.1版本 演示图： 使用教程 1.0版本-仅支持centos7.x 首先从网络上下载此脚本 wget https://disk.gsbok.cn/Shell/yum.sh 如果出现找不到wget命令可以用下面命令 curl -o yum.sh https://disk.gsbok.cn/Shell/yum.sh 下载完成后在终端窗口执行 chmod u+x yum.sh 然后执行 ./yum.sh 然后输入你需要更换yum源的编号回车即可。 1.1版本支持centos5-8一键修改yum源 先从网络上下载此脚本 wget https://disk.gsbok.cn/Shell/yum1.1.sh 如果出现找不到wget命令可以用下面命令 curl -o yum.sh https://disk.gsbok.cn/Shell/yum1.1.sh 下载完成后在终端窗口执行 chmod u+x yum1.1.sh 然后执行 ./yum1.1.sh 之后就是输入自己需要更换的源对应的编号 然后就是此脚本是没有经过太多的测试，如遇到bug可以联系QQ:2913319095 进行解决。 最后修改：2022 年 05 月 12 日 © 允许规范转载 打赏 赞赏作者 支付宝 微信 如果觉得我的文章对你有用，请随意赞赏 本文转自 https://gsbok.cn/66.html，如有侵权，请联系删除。 ","link":"https://zhiyong89.github.io/post/yum-script/"},{"title":"Kubernetes HostAliases添加其他主机别名到Pod","content":"k8s的不同微服务之间可以通过service-name域名来相互访问，通过集群中的coreDNS来完成service-name域名解析。当我们想在Pod上增加一些域名解析时（例如宿主机的主机名），操作DNS模块也不太方便。那么k8s上有没有像linux主机那样，可以直接在/etc/hosts文件中设置域名解析呢？ 1. 在容器镜像中添加/etc/hosts解析（不行） 很容易想到的是，我们把域名记录到容器镜像的/etc/hosts文件，这样容器运行时就可以正确解析了。然而这样是不行的，docker会管理/etc/hosts文件，打到镜像里的域名解析实际并不会起作用。 FROM nginx:latest RUN echo &quot;8.8.8.8 google.com&quot; &gt;&gt; /etc/hosts EXPOSE 80 CMD [&quot;/usr/local/nginx/sbin/nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] # docker run -it nginx /bin/bash Container# cat /etc/hosts 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.17.0.5 8153d35a37e 显然在打镜像的时候配置/etc/hosts是不满足要求的。 2. 在Pod控制器yaml中添加hostAliases解析（行） 我们可以在Pod控制器yaml中通过.spec.hostAliases字段添加主机别名，这个功能是在1.7.x以及以上版本提供的。 apiVersion: v1 kind: Pod metadata: name: hostaliases-pod spec: restartPolicy: Never hostAliases: – ip: &quot;10.1.2.2&quot; hostnames: – &quot;mc.local&quot; – &quot;rabbitmq.local&quot; – ip: &quot;10.1.2.3&quot; hostnames: – &quot;redis.local&quot; – &quot;mq.local&quot; containers: – name: cathosts image: busybox command: – cat args: – &quot;/etc/hosts&quot; # kubectl apply -f hostalias.yaml # kubectl exec hostaliases-pod -- cat /etc/hosts # Kubernetes-managed hosts file. 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet fe00::0 ip6-mcastprefix fe00::1 ip6-allnodes fe00::2 ip6-allrouters 127.0.0.1 foo.local 127.0.0.1 bar.local 10.1.2.2 mc.local 10.1.2.2 rabbitmq.local 10.1.2.3 redis.local 10.1.2.3. mq.local 在yaml配置文件中，增加的几条记录都出现在Pod容器的/etc/hosts文件中了。 3. 参考文章 https://www.centosdoc.com/docker/163.html 本文转自 https://blog.csdn.net/yjk13703623757/article/details/108071067，如有侵权，请联系删除。 ","link":"https://zhiyong89.github.io/post/k8s-HostAliases/"},{"title":"Kubernetes Pod 的服务质量（QoS）","content":" 1. 前言 本页介绍怎样配置 Pod 让其获得特定的服务质量（QoS）类。Kubernetes 使用 QoS 类来决定 Pod 的调度和驱逐策略。 2. QoS 类 Kubernetes 创建 Pod 时就给它指定了下列一种 QoS 类： Guaranteed Burstable BestEffort 创建命名空间 kubectl create namespace qos-example 3. 创建一个 QoS 类为 Guaranteed 的 Pod 对于 QoS 类为 Guaranteed 的 Pod： Pod 中的每个容器，包含初始化容器，必须指定内存请求和内存限制，并且两者要相等。 Pod 中的每个容器，包含初始化容器，必须指定 CPU 请求和 CPU 限制，并且两者要相等。 下面是包含一个容器的 Pod 配置文件。 容器设置了内存请求和内存限制，值都是 200 MiB。 容器设置了 CPU 请求和 CPU 限制，值都是 700 milliCPU： apiVersion: v1 kind: Pod metadata: name: qos-demo namespace: qos-example spec: containers: - name: qos-demo-ctr image: nginx resources: limits: memory: &quot;200Mi&quot; cpu: &quot;700m&quot; requests: memory: &quot;200Mi&quot; cpu: &quot;700m&quot; kubectl create -f https://k8s.io/examples/pods/qos/qos-pod.yaml --namespace=qos-example 查看 Pod 详情： kubectl get pod qos-demo --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Guaranteed。 结果也确认了 Pod 容器设置了与内存限制匹配的内存请求，设置了与 CPU 限制匹配的 CPU 请求。 spec: containers: ... resources: limits: cpu: 700m memory: 200Mi requests: cpu: 700m memory: 200Mi ... status: qosClass: Guaranteed 说明： 如果容器指定了自己的内存限制，但没有指定内存请求，Kubernetes 会自动为它指定与内存限制匹配的内存请求。同样，如果容器指定了自己的 CPU 限制，但没有指定 CPU 请求，Kubernetes 会自动为它指定与 CPU 限制匹配的 CPU请求。 4. 创建一个 QoS 类为 Burstable 的 Pod 如果满足下面条件，将会指定 Pod 的 QoS 类为 Burstable： Pod 不符合 Guaranteed QoS 类的标准。 Pod 中至少一个容器具有内存或 CPU 请求。 下面是包含一个容器的 Pod 配置文件。 容器设置了内存限制 200 MiB 和内存请求 100 MiB。 apiVersion: v1 kind: Pod metadata: name: qos-demo-2 namespace: qos-example spec: containers: - name: qos-demo-2-ctr image: nginx resources: limits: memory: &quot;200Mi&quot; requests: memory: &quot;100Mi&quot; kubectl create -f https://k8s.io/examples/pods/qos/qos-pod-2.yaml --namespace=qos-example 查看 Pod 详情： kubectl get pod qos-demo-2 --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Burstable。 spec: containers: - image: nginx imagePullPolicy: Always name: qos-demo-2-ctr resources: limits: memory: 200Mi requests: memory: 100Mi ... status: qosClass: Burstable 5. 创建一个 QoS 类为 BestEffort 的 Pod 对于 QoS 类为 BestEffort 的 Pod，Pod 中的容器必须没有设置内存和 CPU 限制或请求。 下面是包含一个容器的 Pod 配置文件。 容器没有设置内存和 CPU 限制或请求。 apiVersion: v1 kind: Pod metadata: name: qos-demo-3 namespace: qos-example spec: containers: - name: qos-demo-3-ctr image: nginx kubectl create -f https://k8s.io/examples/pods/qos/qos-pod-3.yaml --namespace=qos-example 查看 Pod 详情： kubectl get pod qos-demo-3 --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 BestEffort。 spec: containers: ... resources: {} ... status: qosClass: BestEffort 6. 创建包含两个容器的 Pod 下面是包含两个容器的 Pod 配置文件。 一个容器指定了内存请求 200 MiB。 另外一个容器没有指定任何请求和限制。 apiVersion: v1 kind: Pod metadata: name: qos-demo-4 namespace: qos-example spec: containers: - name: qos-demo-4-ctr-1 image: nginx resources: requests: memory: &quot;200Mi&quot; - name: qos-demo-4-ctr-2 image: redis kubectl create -f https://k8s.io/examples/pods/qos/qos-pod-4.yaml --namespace=qos-example 查看 Pod 详情： kubectl get pod qos-demo-4 --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Burstable： spec: containers: ... name: qos-demo-4-ctr-1 resources: requests: memory: 200Mi ... name: qos-demo-4-ctr-2 resources: {} ... status: qosClass: Burstable 本文转自 https://blog.csdn.net/qq_34556414/article/details/115675498，如有侵权，请联系删除。 ","link":"https://zhiyong89.github.io/post/k8s-QoS/"},{"title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post $ hexo new &quot;My New Post&quot; More info: Writing Run server $ hexo server More info: Server Generate static files $ hexo generate More info: Generating Deploy to remote sites $ hexo deploy More info: Deployment ","link":"https://zhiyong89.github.io/post/hello-world/"},{"title":"本地搭建的GitLab中开启Pages功能","content":"最近在公司内部，我负责搭建了GitLab服务，开启了从SVN逐步转到GIT的路程。目前自建的GitLab运行状态良好，非常好用。但是默认的GitLab并没有开启Pages服务，这样的话在编写项目说明文档时，就很不方便了。于是自己试着在本地搭建的GitLab中开启Pages功能，下面把开启过程和遇到的问题记录下来，分享给大家。 1、开启GitLab Pages功能 编辑 /etc/gitlab/gitlab.rb文件，修改如下两行 ##! Define to enable GitLab Pages pages_external_url &quot;http://R7102/&quot; gitlab_pages['enable'] = true 注意的这里的pages_external_url，配置的是Pages使用的域名。如果你没有域名，就先随便写个主机名什么的。之后我们可能通过配置Nginx来解决。 ※注意，最好通过 gitlab-ctl restart 重启GitLab，使得GitLab Pages功能生效。 2、安装配置GitLab Runner 为了能够自动发布Pages，我们需要安装GitLab Runner，然后通过GitLab CI做到Pages内容的自动更新。 由于网络环境不稳定，所以建议不要使用yum方式安装Runner，可以点击下面的链接下载GitLab Runner的安装包。 https://packages.gitlab.com/runner/gitlab-runner 安装好之后，就可以通过命令为我们的项目添加runner了，在命令行中输入以下命令，按照提示一步一步完成即可。 gitlab-runner register 其中需要填写URL和Token，可以在GitLab项目的概览-&gt;Runners找到，类似下图： 3、配置Pages服务的CI 在工程根目录下，创建.gitlab-ci.yml文件，文件内容可以通过GitLab内置的模板生产，如下图： 这里选择HTML为例，就生产了以上代码。其中cp -r docs/. .public这句根据需要修改，将docs改为你存放pages页面的路径即可。 这样，当我们每次执行push动作后，就会触发pages的自动部署。将我们的pages页面发布到GitLab Pages服务中。 4、如何访问Pages页面呢？ 如果你有在上面配置开启Pages时配置了域名了，那么直接访问：http://你的Git账号.域名/工程名，例如： http://xiaowang.mypages.com/project01 但是如果你不想那么麻烦还得配置域名的话，这里也有解决方法，由于GitLab Pages服务是部署到Nginx中，我们可以同配置Nginx来通过IP地址访问。 首先要找啊找，找到Pages的发布位置，和GitLab内置Nginx的位置，分别如下： 1、Pages部署目录：/var/opt/gitlab/gitlab-rails/shared/pages 2、内置Nginx目录：/var/opt/gitlab/nginx 然后编辑nginx目录下的conf/gitlab-pages.conf文件，内容如下： server { listen 6869; ## 端口根据需要填写 server_name 10.21.100.200; ## IP根据实际情况填写 server_tokens off; ## Don't show the nginx version number, a security best practice ## Disable symlink traversal disable_symlinks on; access_log /var/log/gitlab/nginx/gitlab_pages_access.log gitlab_access; error_log /var/log/gitlab/nginx/gitlab_pages_error.log; # Pass everything to pages daemon location / { # 指向pages的发布目录 root /var/opt/gitlab/gitlab-rails/shared/pages; index index.html; } # Define custom error pages error_page 403 /403.html; error_page 404 /404.html; } 配置好后重启Nginx：gitlab-ctl restart nginx 访问页面：http://IP:端口/gitlab账号/工程名/public/，例如：http://10.21.100.200:6869/xiaowang/project01/public/#/ 转载于:https://my.oschina.net/doctorlzr1988/blog/3044964 本文转自 https://blog.csdn.net/weixin_34304013/article/details/92671899，如有侵权，请联系删除。 ","link":"https://zhiyong89.github.io/post/gitlab-pages/"},{"title":"K8s 部署 Gitlab CI Runner","content":" K8s 版本：1.20.6 GitLab CI 最大的作用是管理各个项目的构建状态。因此，运行构建任务这种浪费资源的事情交给一个独立的 Gitlab Runner 来做就会好很多，而且 Gitlab Runner 可以安装到不同的机器上 只要在项目中添加一个.gitlab-ci.yml文件，然后添加一个 Runner ，即可进行持续集成 官方文档：Install GitLab Runner | GitLab 1. 介绍 Pipeline：相当于一次构建任务，里面可以包含多个流程，如安装依赖、运行测试、编译、部署测试服务器、部署生产服务器等。任何提交或者 Merge Request 的合并都可以触发 Pipeline 构建 Stages：表示一个构建阶段。一次 Pipeline 中可定义多个 Stages 所有 Stages 会顺序运行，即当一个 Stage 完成后，下一个 Stage 才会开始 只有当所有 Stages 完成后，该构建任务才会成功 如果任何一个 Stage 失败，那么后面的 Stages 不会执行，该构建任务失败 Jobs：表示构建工作，即某个 Stage 里面执行的工作。一个 Stage 中可定义多个 Jobs 相同 Stage 中的 Jobs 会并行执行 相同 Stage 中的 Jobs 都执行成功时，该 Stage 才会成功 如果任何一个 Job 失败，那么该 Stage 失败，即该构建任务失败 Runner：执行 Gitlab CI 构建任务 2. Gitlab Runner gitlab-ci-runner-cm：Runner 镜像所需环境变量 其他选项可在 Pod 中运行gitlab-ci-multi-runner register --help查看 gitlab-ci-token：存放加密的 Gitlab CI runner token http://gitlab.south.com/admin/runners -&gt; K9Qhf4Sh1T7fqxHSWS5s gitlab-ci-runner-scripts：一个用于注册、运行和取消注册 Gitlab CI Runner 的脚本 只有当 Pod 正常通过 Kubernetes（TERM 信号）终止时，才会触发取消注册。如果强制终止 Pod（SIGKILL 信号），Runner 将不会注销自身，必须手动完成对这种被杀死的 Runner 的清理 gitlab-ci-runner：Runner 的 StatefulSet 控制器 通过 K8s 生命周期钩子：开始运行时取消注册所有的同名 Runner；节点丢失时（即 NodeLost 事件）重新注册自己并开始运行；正常停止 Pod 时运行 unregister 命令来取消自己 apiVersion: v1 kind: ServiceAccount metadata: name: gitlab-ci namespace: gitlab --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: gitlab-ci namespace: gitlab rules: - apiGroups: [&quot;&quot;] resources: [&quot;*&quot;] verbs: [&quot;*&quot;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: gitlab-ci namespace: gitlab subjects: - kind: ServiceAccount name: gitlab-ci namespace: gitlab roleRef: kind: Role name: gitlab-ci apiGroup: rbac.authorization.k8s.io --- apiVersion: v1 kind: ConfigMap metadata: labels: app: gitlab-ci-runner name: gitlab-ci-runner-cm namespace: gitlab data: REGISTER_NON_INTERACTIVE: &quot;true&quot; REGISTER_LOCKED: &quot;false&quot; METRICS_SERVER: &quot;0.0.0.0:9100&quot; CI_SERVER_URL: &quot;http://gitlab.gitlab.svc.cluster.local/ci&quot; # * RUNNER_REQUEST_CONCURRENCY: &quot;4&quot; RUNNER_EXECUTOR: &quot;kubernetes&quot; KUBERNETES_NAMESPACE: &quot;gitlab&quot; # * KUBERNETES_PRIVILEGED: &quot;true&quot; KUBERNETES_CPU_LIMIT: &quot;1&quot; KUBERNETES_MEMORY_LIMIT: &quot;1Gi&quot; KUBERNETES_SERVICE_CPU_LIMIT: &quot;1&quot; KUBERNETES_SERVICE_MEMORY_LIMIT: &quot;1Gi&quot; KUBERNETES_HELPER_CPU_LIMIT: &quot;500m&quot; KUBERNETES_HELPER_MEMORY_LIMIT: &quot;100Mi&quot; KUBERNETES_PULL_POLICY: &quot;if-not-present&quot; KUBERNETES_TERMINATIONGRACEPERIODSECONDS: &quot;10&quot; KUBERNETES_POLL_INTERVAL: &quot;5&quot; KUBERNETES_POLL_TIMEOUT: &quot;360&quot; --- apiVersion: v1 kind: Secret metadata: name: gitlab-ci-token namespace: gitlab labels: app: gitlab-ci-runner data: GITLAB_CI_TOKEN: SzlRaGY0U2gxVDdmcXhIU1dTNXMK # echo K9Qhf4Sh1T7fqxHSWS5s | base64 -w0 --- apiVersion: v1 kind: ConfigMap metadata: labels: app: gitlab-ci-runner name: gitlab-ci-runner-scripts namespace: gitlab data: run.sh: | #!/bin/bash unregister() { kill %1 echo &quot;Unregistering runner ${RUNNER_NAME} ...&quot; /usr/bin/gitlab-ci-multi-runner unregister -t &quot;$(/usr/bin/gitlab-ci-multi-runner list 2&gt;&amp;1 | tail -n1 | awk '{print $4}' | cut -d'=' -f2)&quot; -n ${RUNNER_NAME} exit $? } trap 'unregister' EXIT HUP INT QUIT PIPE TERM echo &quot;Registering runner ${RUNNER_NAME} ...&quot; /usr/bin/gitlab-ci-multi-runner register -r ${GITLAB_CI_TOKEN} sed -i 's/^concurrent.*/concurrent = '&quot;${RUNNER_REQUEST_CONCURRENCY}&quot;'/' /home/gitlab-runner/.gitlab-runner/config.toml echo &quot;Starting runner ${RUNNER_NAME} ...&quot; /usr/bin/gitlab-ci-multi-runner run -n ${RUNNER_NAME} &amp; wait --- apiVersion: apps/v1 kind: StatefulSet metadata: name: gitlab-ci-runner namespace: gitlab labels: app: gitlab-ci-runner spec: updateStrategy: type: RollingUpdate replicas: 2 serviceName: gitlab-ci-runner template: metadata: labels: app: gitlab-ci-runner spec: volumes: - name: gitlab-ci-runner-scripts projected: sources: - configMap: name: gitlab-ci-runner-scripts items: - key: run.sh path: run.sh mode: 0755 serviceAccountName: gitlab-ci securityContext: runAsNonRoot: true runAsUser: 999 supplementalGroups: [999] containers: - image: gitlab/gitlab-runner:latest name: gitlab-ci-runner command: - /scripts/run.sh envFrom: - configMapRef: name: gitlab-ci-runner-cm - secretRef: name: gitlab-ci-token env: - name: RUNNER_NAME valueFrom: fieldRef: fieldPath: metadata.name ports: - containerPort: 9100 name: http-metrics protocol: TCP volumeMounts: - name: gitlab-ci-runner-scripts mountPath: &quot;/scripts&quot; readOnly: true restartPolicy: Always 创建： $ kubectl create -f gitlab-runner.yaml $ kubectl -n gitlab get pod NAME READY STATUS RESTARTS AGE gitlab-7b894fcff-mnkb4 1/1 Running 0 69m gitlab-ci-runner-0 1/1 Running 0 2m gitlab-ci-runner-1 1/1 Running 0 2m postgresql-6b6b478f-s6nj7 1/1 Running 0 69m redis-7db89c7d46-fqdr5 1/1 Running 0 69m 结果： 在 http://gitlab.south.com/admin/runners 即可看到两个 Runner 实例 参考：在 Kubernetes 上安装 Gitlab CI Runner-阳明的博客 本文转自 https://www.cnblogs.com/lb477/p/15173133.html，如有侵权，请联系删除。 ","link":"https://zhiyong89.github.io/post/k8s-gitlab-runner/"},{"title":"Hexo + GitHub 搭建个人博客(四) Hexo部署","content":"Hexo 部署 新建仓库 打开 GitHub 网站 登录 新建仓库 绑定 ssh 公钥 生成公钥 ssh-keygen -t ed25519 -C &quot;your_email@example.com&quot; 查看公钥 cat ~/.ssh/id_ed25519.pub 添加公钥 打开 GitHub 设置 &gt; SSH and GPG keys 将生成好的公钥添加进去即可 安装插件 在项目的根目录下执行 npm install hexo-deployer-git -save hexo 配置 git 打开根目录下的 _config.yml 修改一下配置 # Deployment ## Docs: https://hexo.io/docs/one-command-deployment deploy: type: git repo: 你的 github 仓库地址 branch: 你的 git 分支 hexo 部署 github 在项目的根目录下执行 hexo g &amp;&amp; hexo d 好了 此时已经部署到了你的 github 上 打开 仓库的 设置 &gt; Pages 设置你的 github Pages 标签: hexo个人博客 本文转自 https://lyboy6.github.io/blog/2022/09/12/hexo-deploy.html，如有侵权，请联系删除。 ","link":"https://zhiyong89.github.io/post/hexo-blog-4/"},{"title":"Hexo + GitHub 搭建个人博客(二) Hexo主题","content":"前言 hexo-theme-monie 卡片化设计 安装 Git 安装 在项目的根目录下执行 git clone https://gitee.com/lyboy6/hexo-themes-monie.git themes/monie npm 安装 npm install hexo-theme-monie --save 应用主题 修改 hexo 配置文件 _config.yml 把主题改成 monie theme: monie 安装插件 如果你沒有 pug 以及 stylus 的渲染器，请下载安装: npm install hexo-renderer-pug hexo-renderer-stylus --save 代码高亮 highlight: enable: false line_number: true auto_detect: false tab_replace: '' wrap: true hljs: false prismjs: enable: false preprocess: true line_number: true tab_replace: '' 本文转自 https://lyboy6.github.io/blog/2022/09/09/hexo-themes-monie.html，如有侵权，请联系删除。 ","link":"https://zhiyong89.github.io/post/hexo-blog-2/"},{"title":"Hexo + GitHub 搭建个人博客(一) Hexo的搭建","content":"hexo 搭建个人博客 现在的网上的可以发表文章网站有很多地方 (CSDN, 掘金, 简书 ) 等等 相比于 别人的 哪有自己的香 有很多人新手童鞋想要建立自己的博客 博客开源框架 又有很多 不知如何选择 这里推荐就给大家推荐一个 hexo Hexo 是基于 node.js 快速、简洁且高效的博客框架 环境搭建 node.js Node.js® 是一个基于 Chrome V8 引擎 的 JavaScript 运行时环境。 中文官网： https://nodejs.org/zh-cn 下载长期支持版 安装好了之后，打开 cmd 输入 node -v ，如果显示的是下载的版本号那就是安装成功了。 Git Git（读音为/gɪt/）是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理。 也是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件 Git 官网： https://git-scm.com/downloads 安装好了之后，打开 cmd 输入 git --version ，如果显示的 Git 的版本号那就是安装成功了。 注册GitHub账号 因为我们需要把 Hexo 站点托管到 GitHub 上，所以我们需要一个 GitHub 账号。 打开 GitHub 官网，点击 Sigh up，然后输入邮箱、密码、用户名，最后一个按自己需求来写，通过人机验证后，点击 Create account 创建账号就ok了。 现在我们的准备工作 就完成了 接下来 我们就要安装 Hexo 安装Hexo hexo中文官网： https://hexo.io/zh-cn/ 打开我们 cmd 黑窗口 输入命令 npm install hexo-cli -g 安装好后 输入 hexo -v 输入版本号 代表安装成功了 接下来 就是 在你任意文件中 创建 blog 文件夹 然后输入 hexo init blog 初始化文件夹，接着输入 cd blog 将目录切换到 blog 文件夹里,然后输入 npm install 就会在文件夹里生成一些需要的文件。 输入 hexo -h 查看帮助 这样就配置好了，可以输 hexo g 生成静态网页，然后输入 hexo s 打开本地服务器来查看网页 好了 现在的我们的 Hexo 博客已经搭建完毕了 下一节 我们讲 博客的主题配置 标签: hexo个人博客 本文转自 https://lyboy6.github.io/blog/2022/09/09/hexo-init.html，如有侵权，请联系删除。 ","link":"https://zhiyong89.github.io/post/hexo-blog-1/"},{"title":"Hexo + GitHub 搭建个人博客(三) Hexo配置","content":"Hexo 博客配置 你可以 在根目录下 _config_yml 中 修改大部分的配置 网站 参数 描述 title 网站标题 subtitle 网站副标题 description 网站描述 keywords 网站的关键词。支持多个关键词。 author 您的名字 您可以 去 Hexo 的 官方网站 查看更多的配置选项 https://hexo.io/zh-cn/docs/configuration 写作 你可以执行下列命令来创建一篇新文章或者新的页面。 hexo new [layout] &lt;title&gt; 您可以在命令中指定文章的布局（layout），默认为 post ，可以通过修改 _config.yml 中的 default_layout 参数来指定默认布局 布局（layout） Hexo 有三种默认布局：post 、page 和 draft。在创建这三种不同类型的文件时，它们将会被保存到不同的路径；而您自定义的其他布局和 post 相同，都将储存到 source/_posts 文件夹。 布局 路径 描述 post source/_posts 文章 page source 新页面 draft source/_draft 草稿 您可以去 Hexo 的 官网网站 查看更多的配置选项 https://hexo.io/zh-cn/docs/writing 图片资源插件 源（Asset）代表 source 文件夹中除了文章以外的所有文件，例如图片、CSS、JS 文件等。比方说，如果你的 Hexo 项目中只有少量图片，那最简单的方法就是将它们放在 source/ 文件夹中。然后通过类似于 ![](image.jpg) 的方法访问它们。 hexo 的图片 您可以放在图床 服务器 oss 等地方 如果是本地图片 需要怎么引入呢 这里推荐一个 插件 在根目录下执行 npm install hexo-asset-image 新建文章 hexo new post test 这时会在 source 文件创建 test.md 和 test 文件夹 你只需要把图片放在 test 文件夹中 在 test.md 使用 ![](test.png) 完美 这时候图片就可以在本地引入了 标签: hexo个人博客 本文转自 https://lyboy6.github.io/blog/2022/09/11/hexo-config.html，如有侵权，请联系删除。 ","link":"https://zhiyong89.github.io/post/hexo-blog-3/"},{"title":"BRICS-FS-11云计算赛项样题难点解析（私有云部分）","content":"一、配置时间同步服务器chrony 安装chrony服务（默认安装） yum install -y chrony 修改/etc/chrony.conf文件 vim /etc/chrony.conf 服务端（controller）修改以下三行内容 server 192.168.1.21 iburst``allow 192.168.1.0/24``local stratum 10 客户端（compute）修改一行内容 server 192.168.1.21 iburst 重启服务 ~]# systemctl start chronyd.service``~]# systemctl enable chronyd.service 用命令查看是否修改成功 二、将 mencached 最大连接数修改为 2048 将MAXCONN=“1024”改为2048 cd /etc/sysconfig/``vim memcached``PORT=&quot;11211&quot;``USER=&quot;memcached&quot;``MAXCONN=&quot;2048&quot;``CACHESIZE=&quot;64&quot;``OPTIONS=&quot;-l 127.0.0.1,::1,controller&quot; 三、使用 openstack 命令请求一个 token [root@controller ~]# openstack token issue 如上所示，id即为token值 四、openstack 命令将镜像****上传到 openstack 平台镜像命名为 cirros 五、openstack中将控制节点的计算资源也加入集群 把compute节点的名称和IP都改成controller节点的名称和IP cat /etc/iaas-openstack/openrc.sh 在Controller节点安装nova-compute服务 bash /usr/local/bin/iaas-install-nova-compute.sh 安装到最后需要输入Controller节点密码，输入后弹出该表格即是成功！ 修改Controller节点的nova配置文件 因为已经把Controller节点资源加入到云平台，所有创建云主机也可能基于在Controller节点上。为了防止云主机无系统请记得更改nova配置文件 cat /etc/nova/nova.conf``[libvirt]``virt_type=qemu ##在[libvirt]下添加此行即可 六、OpenStack运维部 1、创建云主机外部网络 ext-net，子网为 ext-subnet,云主机浮动 IP 可用网段为 172.18.x.100~172.18.x.200，网关为 172.18.x.1。 openstack network create --provider-network-type=vlan \\``&gt; --provider-physical-network=provider --provider-segment=100 \\``&gt; --external --share ext-net openstack subnet create ext-subnet --network ext-net \\``&gt; --gateway 172.18.6.1 --allocation-pool start=172.18.6.100,end=172.18.6.200 \\``&gt; --subnet-range 172.18.6.0/16 2、创建云主机内部网络 int-net1，子网为 int-subnet1，云主机子网 IP 可用网段为 10.0.0.100~10.0.0.200，网关为 10.0.0.1；创建云主机内部网int-net2，子网为 int-subnet2，云主机子网 IP 可用网段为 10.0.1.100 ~ 10.0.1.200，网关为 10.0.1.1。 openstack network create int-net1 openstack subnet create int-subnet1 --network int-net1 \\``&gt; --allocation-pool start=10.0.0.100,end=10.0.0.200 \\``&gt; --geteway 10.0.0.1 --subnet-range 10.0.0.0/8 openstack network create int-net2 openstack subnet create int-subnet2 \\``&gt; --subnet-range 10.0.0.0/8 \\``&gt; --gateway 10.0.0.1 \\``&gt; --network int-net2 \\``&gt; --allocation-pool start=10.0.0.100,end=10.0.0.200 3、添加名为 ext-router 的路由器，添加网关在 ext-net 网络，添加内部端口到 int-net1 网络，完成内部网络 int-net1 和外部网络的连通 [root@controller ~]# openstack router add subnet ext-router ext-subnet` `[root@controller ~]# openstack router add subnet ext-router int-subnet1 4、在 keystone 中创建用户 testuser，密码为 password。 openstack domain list``openstack user create testuser --password password --domain demo 5、使用 命令修改 testuser 密码为 000000，并查看 testuser 的详细信息。添加将该用户添加到 admin 项目并赋予普通用户权限，完成后测试登录。 openstack user set testuser --password 000000``openstack user show testuser``openstack user set testuser --project admin openstack role add --project admin --user testuser user``openstack user show testuser 最后的 openstack role add --project admin --user testuser user 是测试这条命令是否正确且生效 6、使用命令列出服务目录和端点，查看 glance 服务的端点 https://www.talkwithtrend.com/Question/432247 openstack catalog list openstack catalog show glance 7、使用 glance 相关命令，上传镜像，源使用 CentO S_6.5_x86_64_XD.qcow2，名字为 testone，然后使用 openstack 命令修改这个镜像名改为 examimage，然后给这个镜像打一个标签，标签名字为 lastone 改完后使用 openstack 命令查看镜像列表. openstack image create testone --container-format bare \\``&gt; --disk-format qcow2 --file /root/CentO S_6.5_x86_64_XD.qcow2 \\``&gt; --public``openstack image set testone --name examimage openstack image set examimage --tag lastone``openstack image list``openstack image show examimage 8、进入到glance 后端存储目录中，使用 qemu 命令查看任意的一个镜像信息 glance后端存储目录为 /var/lib/glance/images/ cd /var/lib/glance/images/``qemu-img info &lt;镜像文件名&gt; 9、使用 du 命令查看 nova 主配置文件大小 cd /etc/nova/``du -h nova.conf 10、创建一个卷类型，然后创建一块带这个卷类型标识的云硬盘，查询其详细信息。 openstack命令创建卷相关 https://cloud.tencent.com/developer/article/1644193 openstack volume type create testvolume``openstack volume type list openstack volume create --type testvolume --size 1 testone openstack volume list``openstack volume show testone 11、将该云硬盘挂载到虚拟机中，将该云硬盘格式化为 xfs。创建一个文件文件名为工位号内容为工位号，然后将该云硬盘卸载，使用 openstack 命令将该云硬盘修改为只读状态，再次挂载后查看是否存在原始文件，然后再次向该云硬盘中创建一个文件，文件名为工位号_02。 创建云主机 openstack server create --image examimage --flavor small \\``&gt; --nic net-id=&lt;内网网络id&gt; &lt;云主机名称&gt; openstack network list``openstack subnet list``openstack server list``openstack server show &lt;云主机名称&gt; 挂载磁盘到云主机 [root@controller ~]# openstack server add volume cirrors testone 进云主机内查看（vdc） lsblk` `mkfs.xfs /dev/vdc mkdir vdc``mount /dev/vdc /opt/vdc/ cd /opt/vdc/ vim 6 6 x保存 从云主机上卸载云硬盘 [root@controller ~]# openstack server remove volume centos_7.5 testone 将云硬盘设置为只读 [root@controller ~]# openstack volume set --read-only testone 再次挂载云硬盘 [root@controller ~]# openstack server add volume centos_7.5 testone 12、使用命令创建一个 5GB 的云硬盘，名称为 disk-2，将云硬盘挂载到云虚拟机内，然后格式化为 ext4，挂载到虚拟机的 /mnt/ 目录下，使用 df -h 将命令和返回信息提交到答题框 openstack volume create --size 5 disk-2``openstack server add volume create centos_7.5 disk-2 mkfs.ext4 /dev/vd` `mount /dev/vde /mnt/``df -hT 13、将该云硬盘使用命令卸载，使用命令将该云硬盘扩容到10GB使用命令将云硬盘挂载到云主机上，将命令及返回信息提交到答题框。 进入云主机使用命令扩容文件系统，扩容后再次挂载到 /mnt/ [root@controller ~]# openstack server remove volume centos_7.5 disk-2` `[root@controller ~]# openstack volume set disk-2 --size 10``[root@controller ~]# openstack server add volume centos_7.5 disk-2 卸载之前的vde ，重新格式化vdf，然后挂载 14、使用 swift 相关命令，创建一个容器，然后往这个容器中上传一个文件（文件可以自行创建），上传完毕后，使用命令查看容器 swift相关命令 https://www.likecs.com/show-204426264.html?sc=1000 swift post c1``openstack container list openstack object list c1``swift upload c1 /etc/hosts``openstack object list c1 15、使用命令创建名称为 group_web 的安全组该安全组的描述为工位号，为该安全组添加一条规则允许任意 ip 地址访问 web 流量，完成后查看该安全组的详细信息 openstack security group create group_web --description 6 openstack security group rule create group_web --protocol tcp --egress openstack security group show group_web 16、使用命令将int-net1网络设置为共享，然后查看int-net1网络的详细信息 openstack subnet set int-net1 --shared` `openstack subnet show int-net1 17、使用dashboard界面使用centos7.5镜像创建一台云主机，云主机命名为test-01，使用命令查看浮动 IP 地址池，使用命令创建一个浮动 IP，然后将浮动IP 绑定到云主机上 openstack floating ip create ext-net [root@controller ~]# openstack server add floating ip centos_7.5 192.168.1.105 18、使用 opentack 命令利用 centos7.5 镜像创建一台云主机，连接 int-net1 网络，云主机名称为 test-02。创建成功后使用命令查看云主机详细信息，确定该云主机是处于计算节点还是控制节点。如果云主机处于控制节点上请将其冷迁移到计算节点，如果如果云主机处于计算节点上请将其冷迁移到控制节点。 openstack云主机冷/热迁移相关 https://blog.csdn.net/tony_vip/article/details/123733056 #创建云主机；连接网络；查看详细信息；步骤同上。 冷迁移步骤 1.关闭虚拟机 2.找到虚拟机位于 /var/lib/nova/instances 下文件 3.将虚拟机的文件全部 copy 到目标主机的相同位置下 4.修改用户组 5.更新数据库中 host,node 字段为目标主机的名字 6.重启目标主机的 nova-compute服务 操作记录 1、显示运行的虚机 openstack server list 2、关闭虚机，并切换到 /var/lib/nova/instances /目录下 [root@controller ~]# openstack server stop centos_7.5 前面图中可以看出虚机在compute节点 [root@compute ~]# cd /var/lib/nova/instances/``[root@compute instances]# ls``38342c2e-b392-44c1-93a6-83fc11d5113c _base compute_nodes locks 3.将文件copy到目标主机的对应位置下，并修改其权限 [root@compute instances]# scp -r /var/lib/nova/instances/* controller:/var/lib/nova/instances/ [root@controller instances]# chown -R nova:nova 38342c2e-b392-44c1-93a6-83fc11d5113c/ _base/ compute_nodes locks/ 4、修改数据库中的字段 mysql -uroot -p000000``MariaDB [(none)]&gt; use nova;``MariaDB [nova]&gt; update instances set host='controller', node='controller' where uuid='38342c2e-b392-44c1-93a6-83fc11d5113c'; 5、查询虚机所在的主机 openstack server list``openstack server show centos_7.5 到这里就结束了，如果有帮助的话可以支持一下博主，下面按钮投喂博主。 本文转自 https://mp.weixin.qq.com/s/5vy8sOsPCDLlpCGB1MNlbw，如有侵权，请联系删除。 ","link":"https://zhiyong89.github.io/post/BRICS/"},{"title":"openstack运维题","content":"一、私有云运维 1、cinder创建硬盘 2、heat安装 3、镜像管理 4、网络管理 5、云主机管理 6、快照管理 7、数据库安装 1、安装完后登入数据库中创建chinaskilldb库，在chinaskilldb库中创建表testable (id int not null primary key,Teamname varchar(50), remarks varchar(255))，在表中插入记录(1,“cloud”,“chinaskill”)。 2、将memcached的缓存大小从64Mib改成256Mib。 3、使用命令 创建用户chinaskill，并设置Administrators限权 完成后提交控制节点的用户名、密码和IP地址到答题框。 8、keystone安装 在controller节点上使用iaas-install-keystone.sh 脚本安装Keystone服务。创建一个用户chinaskill完成后提交控制节点的用户名、密码和IP地址到答题框。 9、heat模板管理 在openstack私有云平台上，在/root目录下编写模板server.yaml，创建名为“m1.flavor”、 ID 为 1234、内存为1024MB、硬盘为20GB、vcpu数量为 1的云主机类型。完成后提交控制节点的用户名、密码和IP地址到答题框。（在提交信息前请准备好yaml模板执行的环境） [root@controller ~]# cat server.yaml heat_template_version: 2014-10-16 description: deploy a single flavor resources: server: type: OS::Nova::Flavor properties: disk: 20 flavorid: 1234 name: m1.flavor ram: 1024 vcpus: 1 [root@controller ~]# openstack stack create -t server.yaml test1 10、openstack动态调整云主机类型 动态将云主机类型调整为2核2G内存 云主机类型。 修改controller和各个computer节点的nova.cnf文件 [root@controller ~]# vim /etc/nova/nova.conf [DEFAULT] allow_resize_to_same_host=True scheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter [root@controller ~]# systemctl restart openstack-nova-api.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service [root@compute ~]# systemctl restart libvirtd.service openstack-nova-compute.service 需要把云主机关机，然后进行调整云主机类型，需要进行确认。 10、swift作为glance镜像服务的后端存储 使用swift对象存储作为glance镜像服务的后端存储。 [root@controller ~]# cat /etc/glance/glance-api.conf |egrep -v '^$|^#' [glance_store] stores = glance.store.swift.Store default_store = swift swift_store_region = RegionOne swift_store_endpoint_type = internalURL swift_store_container = glance swift_store_large_object_size = 5120 swift_store_large_object_chunk_size = 200 swift_store_create_container_on_put = True swift_store_multi_tenant = True swift_store_admin_tenants = service swift_store_auth_address = http://controller:5000/v3.0/ swift_store_user = glance swift_store_key = 000000 上传镜像之后，验证 [root@controller ~]# openstack container list 11、云平台安全策略提升，openstack中http转https [root@controller ~]# yum install -y mod_wsgi mod_ssl [root@controller ~]# vim /etc/httpd/conf.d/ssl.conf 75行 SSLProtocol all -SSLv2 -SSLv3 删除-SSLv3 [root@controller ~]# vim /etc/openstack-dashboard/local_settings 49行起 CSRF_COOKIE_SECURE = True 取消注释 SESSION_COOKIE_SECURE = True 取消注释 USE_SSL = True 添加该行 SESSION_COOKIE_HTTPONLY = True 添加该行 [root@controller ~]# systemctl restart httpd memcached 272 history 可以用https访问 https://192.168.100.10/dashboard 12、python 程序对接openstack api 在 controller 节点的/root 目录下编写 Python 程序 create_flavor.py 文件，对接 openstack api,，创建一个云主机类型 centos7：vcpu 为 2 个、内存为 2048M、硬盘大小 40G。 解压，配Python源 1、yum install -y python3 2、pip3 install certifi-2019.11.28-py2.py3-none-any.whl 3、pip3 install urllib3-1.25.11-py3-none-any.whl 4、pip3 install idna-2.8-py2.py3-none-any.whl 5、pip3 install chardet-3.0.4-py2.py3-none-any.whl 6、pip3 install requests-2.24.0-py2.py3-none-any.whl [root@controller ~]# cat create_flavor.py import requests import json osurl = &quot;http://192.168.100.10&quot; body={ &quot;auth&quot;:{ &quot;identity&quot;:{ &quot;methods&quot;:[&quot;password&quot;], &quot;password&quot;:{ &quot;user&quot;:{ &quot;id&quot;:&quot;d618c8ee2d234997889b62ee18562ee0&quot;, &quot;password&quot;:&quot;000000&quot; } } }, &quot;scope&quot;:{ &quot;project&quot;:{ &quot;id&quot;:&quot;5e7709cbbd7c4c0e8733c3f7360d1d38&quot; } } } } headers = {} def get_token(): url = osurl+&quot;:5000/v3/auth/tokens&quot; re = requests.post(url,headers=headers,data=json.dumps(body)).headers[&quot;X-Subject-Token&quot;] return re def flavor_create(): url = osurl+&quot;:8774/v2.1/flavors&quot; headers[&quot;X-Auth-Token&quot;] = get_token() body = { &quot;flavor&quot;:{ &quot;name&quot;: &quot;centos7&quot;, &quot;id&quot;: 302, &quot;vcpus&quot;: 2, &quot;ram&quot;: 2048, &quot;disk&quot;: 40, } } re = requests.post(url,data=json.dumps(body),headers=headers).json() print(re) return re flavor_create() 13、创建云主机命令 #1.1、创建外网 openstack network create --share --external --provider-physical-network provider --provider-network-type flat pb1 # --share 允许所有项目都可以使用该网络 # --external 定义连通外部的虚拟网络 # --provider-physical-network 指定物理网络的提供者，由ml2_conf.ini文件的flat_networks确定 # --provider-network-type flat 映射到虚拟主机的网卡eth0，由linuxbridge_agent.ini文件中的physical_interface_mappings确定 ##openstack network create --share --external --provider-physical-network provider --provider-network-type flat provider #1.2、创建外网子网 openstack subnet create --network pb1 --allocation-pool start=192.168.121.100,end=192.168.121.200 --gateway 192.168.121.2 --dns-nameserver 114.114.114.114 --subnet-range 192.168.121.0/24 pb1-sn1 #2.1、创建内网 openstack network create --share --internal --provider-physical-network provider --provider-network-type vlan sn-1 #2.2 创建内网的子网 openstack subnet create --network sn-1 --allocation-pool start=10.10.10.100,end=10.10.10.200 --gateway 10.10.10.1 --dns-nameserver 114.114.114.114 --subnet-range 10.10.10.0/24 sn-s1 #3、上传镜像 glance image-create --name centos7.5 --disk-format=qcow2 --container-format=bare --visibility=public --file /opt/iaas/images/CentOS_7.5_x86_64_XD.qcow2 #4、创建实例-云主机类型 openstack flavor create --id 0 --vcpus 2 --ram 2048 --disk 20 m1 #5、创建云主机 openstack server create --flavor m1 --image centos7.5 --nic net-id='61267aba-282b-4802-8429-3e94044145b9' c1 14、heat模板管理2 14.1 创建网络模板 编写Heat模板create_net.yaml，创建名为Heat-Network网络，选择不共享；创建子网名为Heat-Subnet，子网网段设置为10.20.2.0/24，开启DHCP服务，地址池为10.20.2.20-10.20.2.100。模板内容如下： [root@controller ~]# cat create_net.yaml heat_template_version: 2014-10-16 description: Generated template resources: network_1: type: OS::Neutron::Net properties: admin_state_up: true name: Heat-Network shared: false subnet_1: type: OS::Neutron::Subnet properties: allocation_pools: - end: 10.20.2.100 start: 10.20.2.10 cidr: 10.20.2.0/24 enable_dhcp: true host_routes: [] ip_version: 4 name: Heat-Subnet network_id: get_resource: network_1 [root@controller ~]# openstack stack create -t create_net.yaml test2 [root@controller ~]# openstack network list 14.2 创建用户模板 编写Heat模板create_user.yaml，创建名为heat-user的用户，属于admin项目包，并赋予heat-user用户admin的权限，配置用户密码为123456。模板内容如下： [root@controller ~]# cat create_user.yaml heat_template_version: 2014-10-16 resources: user: type: OS::Keystone::User properties: name: heat-user password: &quot;123456&quot; domain: demo default_project: admin roles: [{&quot;role&quot;: admin, &quot;project&quot;: admin}] [root@controller heat]# openstack stack create -t create_user.yaml test-user +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | id | dc70f9dc-f784-4eff-beae-f541a97b455d | | stack_name | test-user | | description | No description | | creation_time | 2021-08-05T07:52:00Z | | updated_time | None | | stack_status | CREATE_IN_PROGRESS | | stack_status_reason | Stack CREATE started | +---------------------+--------------------------------------+ [root@controller heat]# openstack user list +----------------------------------+-------------------+ | ID | Name | +----------------------------------+-------------------+ | 11cb4d06948046dc86b660b94ece5c08 | chinaskill | | 270b1696605349aab7d3e7e091ba92c6 | swift | | 3d37d0c087df482480c8b6510081187e | placement | | 451764b10f18410db820aa6b82964b0d | cinder | | 60b741e3d6074174a4c0bf7a4760c4eb | neutron | | 68e9e28cea1e4438b707282343368a10 | heat_domain_admin | | 74dd9fd47f244ca6be5319b33116187a | heat | | 7f85ba56fc7e4717a93ee7d543d97447 | c1 | | 8dc6d5efeaf84b58bdfc6a128ae1a413 | glance | | 9fd039c7fed84df2b805c5b4b81351ee | heat-user | | a4056eb015294f98b2dce60f0c4b24b1 | nova | | ab90102932f34d5f83da5f59ca19df08 | demo | | f6bec50fd4c84560a81bff3ec4267224 | admin | +----------------------------------+-------------------+ 二、容器云运维 1、安装 节点角色 主机名 VCPUS 内存 磁盘 master、harbor、CI/CD master 4 8 100G worker node node 4 8 100G 安装 Docker CE 和 Docker Compose[2 分] 使用提供的 centos7.5-paas 镜像启动两台云主机 master 和 node，flavor 如上表所示。在master、node 各节点中分别安装 DockerCE 和 docker-compose。完成后提交 master 节点的用 户名、密码和 IP 到答题框 1.1 虚拟机准备 准备两台虚拟机，一个master ,一个node节点 192.168.121.21/22 1) 关闭SELinux 所有节点关闭SELinux： sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config setenforce 0 2) 所有节点关闭防火墙： systemctl stop firewalld.service systemctl disable firewalld.service 1.2 初始配置 上传镜像（集群环境则使用包含PASS资源包的云主机镜像） 1、配置域名解析 2、配置yum源 1.3 执行脚本 master节点： 1. k8s_harbor_install.sh 安装docker-CE harbor load 2. k8s_image_push.sh 本地镜像上传到harbor仓库 3. k8s_master_install.sh master安装k8s node节点： 4. k8s_node_install.sh node安装docker-ce harbor 加入k8s集群 [root@master opt]# docker -v Docker version 19.03.13, build 4484c46d9d [root@master opt]# docker-compose -v docker-compose version 1.25.5, build 8a1c60f6 补充： 1、复制docker-compose [root@master ~]# scp /usr/local/bin/docker-compose node:/usr/local/bin/docker-compose 2、node节点加入k8s集群 [root@node ~]# sh token.sh [root@master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready master 10m v1.18.1 node Ready &lt;none&gt; 40s v1.18.1 2、docker-compose编排 2.1 wordpress编排 wordpress: 建博客工具 在node节点上从仓库中拉取mysql:latest和wordpress:latest镜像，创建/root/wproject/docker-compse.yaml文件，编排部署wordpress，并设置 restart策略。 node节点拉取镜像 [root@node ~]# mkdir wproject/ [root@node wproject]# cat docker-compose.yaml version: '2' services: mysql: image: 192.168.121.111/library/mysql:5.6 expose: - &quot;3306&quot; restart: always environment: - MYSQL_ROOT_PASSWORD=123456 wordpress: image: 192.168.121.111/library/wordpress ports: - &quot;80:80&quot; restart: always environment: - WORDPRESS_DB_HOST=mysql - WORDPRESS_DB_USER=root - WORDPRESS_DB_PASSWORD=123456 [root@node ~]# docker-compose -f docker-compose.yaml up 浏览器访问192.168.121.22 ##停止并删除wordpress容器 [root@node ~]# docker-compose -f docker-compos.yaml stop [root@node ~]# docker rm `docker ps -qa` 2.2 owncloud编排 owncloud： 云网盘 在node1节点上从仓库中拉取mysql:latest和owncloud:latest镜像，创建/root/wproject/docker-compse.yaml文件，编排部署owncloud，并设置 restart策略。 在node1节点上停止并删除上述部署的owncloud容器。 1）拉取镜像 2）创建docker-compose.yaml [root@node ~]# mkdir ownproject [root@node ~]# cd wproject/ [root@node ownproject]# vim docker-compose.yaml owncloud: image: 192.168.121.21/library/owncloud restart: always links: - mysql:mysql ports: - 80:80 mysql: image: 192.168.121.21/library/mysql restart: always environment: MYSQL_ROOT_PASSWORD: 123456 2.3 lychee 云相册 在node1节点上从仓库中拉取mysql:latest和lychee:latest镜像，创建/root/wproject/docker-compse.yaml文件，编排部署lychee，并设置 restart策略。 在node1节点上停止并删除上述部署的lychee容器。 [root@node lychee]# cat docker-compose.yaml mysql: image: 192.168.121.21/library/mysql environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: lychee MYSQL_USER: lychee MYSQL_PASSWORD: 123456 ports: - &quot;3306:3306&quot; restart: always lychee: image: 192.168.121.21/library/lychee ports: - 80:80 links: - &quot;mysql:mysql&quot; environment: WONCLOUD_DB_NAME: lychee WONCLOUD_DB_USER: lychee WONCLOUD_DB_PASSWORD: 123456 restart: always 3、web全容器化应用系统部署 1) 使用提供的gmall.tar.gz压缩包，压缩包包含部署的所有文件，包含Dockerfile和docker-compose.yml，docker-compose程序文件 2) 将6个dockerfile分别构建成镜像。 jar:2021 nginx:2021 mysql:2021 redis:2021 zk:2021 kafka:2021 3) 编写docker-compose.yaml文件， 使用docker-compose up 编排部署 4）测试访问 4、CI /CD 持续集成/持续交付 该公司决定采用GitLab + Jenkins来构建CICD环境，以缩短新功能开发上线周期，及时满足客户的需求，实现DevOps的部分流程，来减轻部署运维的负担，可视化容器生命周期管理、应用发布和版本迭代更新，请完成GitLab + Jenkins的CICD环境部署： 1.从私有仓库中拉取gitlab:latest镜像，创建/root/devops/docker-compose.yaml文件，启动gitlab服务，实现web浏览器正常访问gitlab服务。 gitlab，版本控制工具 [root@node devops]# pwd /root/devops [root@node devops]# cat docker-compose.yaml version: '2' services: gitlab: image: 192.168.121.21/library/gitlab-ce:12.9.2-ce.0 container_name: &quot;gitlab&quot; restart: unless-stopped privileged: true hostname: 'gitlab' environment: TZ: 'Asia/Shanghai' GITLAB_OMNIBUS_CONFIG: | external_url 'http://192.168.121.22' gitlab_rails['time_zone'] = 'Asia/Shanghai' gitlab_rails['gitlab_shell_ssh_port'] = 22 ports: - &quot;80:80&quot; - '443:443' - '2222:22' volumes: - /opt/gitlab/config:/etc/gitlab - /opt/gitlab/data:/var/opt/gitlab - /opt/gitlab/logs:/var/log/gitlab ### docker-compose up ###192.168.121.22访问 2.创建gitlab用户（用户名：Chinaskill），创建gitlab项目（项目名：ChinaskillProject），实现通过SSH链接克隆项目。 1） 改为中文界面 2） 管理中心---&gt;用户---&gt;新用户--- 设定密码 3） 登录新用户，-改中文界面--创建项目 4） 添加密钥 [root@node .ssh]# cat /root/.ssh/id_rsa.pub 5）登录方式 [root@node ~]# vim /root/.ssh/config HOST 192.168.121.22 Port 2222 IdentityFile ~/.ssh/id_rsa User git [root@node ~]# mkdir Chinaskill [root@node ~]# cd Chinaskill/ 克隆项目 5.1） [root@node Chinaskill]# git clone git@192.168.121.22:gitlab/chinaskillproject.git ##克隆地址 可以在项目中找到 5.2） [root@node Chinaskill]# git clone ssh://192.168.121.22/gitlab/chinaskillproject.git 3.从私有镜像仓库中拉取Jenkins镜像，在cicd-node节点上运行部署Jenkins容器，将容器的8080端口映射为宿主机的8080端口。 [root@node ~]# docker run -itd -p 8080:8080 -p 50000:50000 -v jenkis-data:/var/jenkins_home \\ &gt; -v /var/run/docker.sock:/var/run/docker.sock 192.168.121.21/library/jenkins 262a6eb4a043628c42ba3ffcfb0c0edbd96ebc46f3c92cc556132d732a232c43 [root@node ~]# cd /var/lib/docker/volumes/jenkis-data/_data [root@node _data]# cat secrets/initialAdminPassword 5.在node节点/root目录下克隆ChinaskillProject项目，修改项目中的index.html文件（/root/ChinaskillProject/templates/index.html）中的“Hello,word!”修改为“Hello,ChinaSkill!”，提交并推送（push）代码。 1) 进入到项目的目录 [root@node ~]# cd - /root/Chinaskill/chinaskillproject [root@node chinaskillproject]# ls README.md [root@node chinaskillproject]# pwd /root/Chinaskill/chinaskillproject [root@node chinaskillproject]# vi index.html hello，chinaskill [root@node chinaskillproject]# git add index.html [root@node chinaskillproject]# git commit -a -m &quot;first upload index.html&quot; [root@node chinaskillproject]# git config --global user.name gitlab [root@node chinaskillproject]# git config --global user.email gitlab@qq.com [root@node chinaskillproject]# git commit -a -m &quot;first upload index.html&quot; [root@node chinaskillproject]# git push 查看项目中是否存在文件。 5、k8s运维 【题目1】Pod管理 在master节点/root目录下编写yaml文件nginx.yaml，具体要求如下： （1）Pod名称：nginx-pod； （2）命名空间：default； （3）容器名称：mynginx； （4）镜像：nginx；拉取策略：IfNotPresent； （5）容器端口：80。 完成后使用该yaml文件创建Pod，并提交master节点的用户名、密码和IP到答题框。 [root@master ~]# vim nginx.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: mynginx name: nginx-pod namespace: default spec: containers: - image: 192.168.121.21/library/nginx:latest name: mynginx ports: - containerPort: 80 imagePullPolicy: IfNotPresent [root@master ~]# kubectl create -f nginx.yaml 【题目2】Deployment管理 在master节点/root目录下编写yaml文件nginx-deployment.yaml，具体要求如下： （1）Deployment名称：nginx-deployment； （2）命名空间：default； （3）Pod名称：nginx-deployment，副本数：2； （4）网络：hostNetwork； （5）镜像：nginx； （6）容器端口：80 完成后使用该yaml文件创建Deployment，并提交master节点的用户名、密码和IP到答题框。 apiVersion: apps/v1 ##版本号/pod资源 kind: Deployment ##类型/控制器 metadata: ##数据标签 name: nginx-deployment namespace: default labels: ##子标签 app: nginx-deployment ##业务容器 spec: ###容器的详细定义 replicas: 2 ##副本集 selector: ##选择器 matchLabels: ##匹配标签 app: nginx-deployment ##对应业务标签 template: ##模板 metadata: labels: app: nginx-deployment spec: hostNetwork: true containers: - name: nginx-deployment ##对应业务容器 image: 192.168.121.21/library/nginx:latest #镜像 ports: - containerPort: 80 ##容器端口 【题目3】ReplicaSet管理 在master节点/root目录下编写yaml文件replicaset.yaml，具体要求如下： （1）Replicaset名称：nginx； （2）命名空间：default； （3）副本数：3； （4）镜像：nginx。 完成后使用该yaml文件创建ReplicaSet，并提交master节点的用户名、密码和IP到答题框。 apiVersion: v1 kind: ReplicationController metadata: name: nginx namespace: default spec: replicas: 3 selector: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: 192.168.121.21/library/nginx:latest 【题目4】健康检查 在master节点/root目录下编写yaml文件liveness_httpget.yaml，具体要求如下： （1）Pod名称：liveness-http； （2）命名空间：default； （3）镜像：nginx；端口：80； （4）容器启动时运行命令“echo Healty &gt; /usr/share/nginx/html/healthz”； （5）httpGet请求的资源路径为/healthz，地址默认为Pod IP，端口使用容器中定义的端口名称HTTP； （6）启动后延时30秒开始运行检测； （7）每隔3秒执行一次liveness probe。 完成后使用该yaml文件创建Pod，并提交master节点的用户名、密码和IP到答题框。 apiVersion: v1 kind: Pod metadata: name: liveness-http namespace: default spec: containers: - name: nginx image: 192.168.100.10/library/nginx:latest ports: - containerPort: 80 args: - /bin/sh - -c - echo Healty &gt; /usr/share/nginx/html/healthz livenessProbe: httpGet: path: /healthz port: http initialDelaySeconds: 30 periodSeconds: 3 三、公有云 任务1 基础设施构建 1.按照1核CPU、4G内存、40G硬盘创建两台名为ChinaSkill-node-1和ChinaSkill-node-2的云主机。 2.在公有云中完成一个名为intnetX的内部网络为192.168.X.0的云主机网络区域配置，将该网络网关设为192.168.X.254，使得ChinaSkill-node-1和ChinaSkill-node-2接入内部网络，并使云主机可以正常接入公共外部网络。 3.创建一个40G的文件块存储disk-1，并将云硬盘格式化为EXT4格式，挂载到ChinaSkill-node-2的/nfs/code目录下。 ","link":"https://zhiyong89.github.io/post/openstack_Om/"},{"title":"测试","content":"介绍 father https://zhiyong0389.github.io/father/ 引用视频 html的方式 引用音频 html的格式 引用图片 markdown的方式 html的格式 微信公众号图片引用测试 ","link":"https://zhiyong89.github.io/post/test-article/"},{"title":"Hello Gridea","content":"👏 欢迎使用 Gridea ！ ✍️ Gridea 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ... Github Gridea 主页 示例网站 特性👇 📝 你可以使用最酷的 Markdown 语法，进行快速创作 🌉 你可以给文章配上精美的封面图和在文章任意位置插入图片 🏷️ 你可以对文章进行标签分组 📋 你可以自定义菜单，甚至可以创建外部链接菜单 💻 你可以在 Windows，MacOS 或 Linux 设备上使用此客户端 🌎 你可以使用 𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌 或 Coding Pages 向世界展示，未来将支持更多平台 💬 你可以进行简单的配置，接入 Gitalk 或 DisqusJS 评论系统 🇬🇧 你可以使用中文简体或英语 🌁 你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力 🖥 你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步 🌱 当然 Gridea 还很年轻，有很多不足，但请相信，它会不停向前 🏃 未来，它一定会成为你离不开的伙伴 尽情发挥你的才华吧！ 😘 Enjoy~ ","link":"https://zhiyong89.github.io/post/hello-gridea/"}]}